{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3f539d4",
   "metadata": {},
   "source": [
    "# Chapter 7 Ensemble Learning and Random Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b475048c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load moons dataset and split into training and testing set\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = make_moons(n_samples=10000, noise=0.15)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3d9360ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression()),\n",
       "                             ('rf', RandomForestClassifier()), ('svc', SVC())])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9c34c087",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.8785\n",
      "RandomForestClassifier 0.99\n",
      "SVC 0.991\n",
      "VotingClassifier 0.992\n"
     ]
    }
   ],
   "source": [
    "# Examine each classifier's accuracy on test set\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564f1adb",
   "metadata": {},
   "source": [
    "Voting classifier slightly outperforms all the individual classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20985270",
   "metadata": {},
   "source": [
    "# Bagging and Pasting in Scikit-Learn\n",
    "\n",
    "Getting a diverse set of classifiers\n",
    "- Use very different training algorithms\n",
    "- Use same training algorithm for every predictor AND train them on different random subsets of the training set\n",
    "    - Bagging: sampling WITH REPLACEMENT (bootstrap = True)\n",
    "    - Pasting: sampling WITHOUT REPLACEMENT (bootstrap = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ece51416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bagging\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8418b3dd",
   "metadata": {},
   "source": [
    "NOTES\n",
    "- BaggingClassifier performs soft voting (instead of hard voting) if base classifier can estimate probabilities (= case for DecisionTreeClassifier)\n",
    "- n_jobs: number of CPU cores to use for training and predictions (-1 if Scikit-Learn should use all available cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80133559",
   "metadata": {},
   "source": [
    "# Out-of-Bag (oob) Evaluations\n",
    "With bagging, some instances may not be sampled at all. Instances that are not sampled are called out-of-bag(oob) instances.\n",
    "The predictor never sees the oob instances during training, the predictor can be evaluated on these instances without a separate test set. The ensemble can be evaluated by averaging the oob evaluations of each predictor.\n",
    "In Scikit-Learn, passing oob_score = True when creating a BaggingClassifier requests an automatic oob evaluation after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6eed55ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9885"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pass oob_score = True - creating BaggingClassifier\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500, \n",
    "    bootstrap=True, n_jobs=-1, oob_score=True)\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b822b5e",
   "metadata": {},
   "source": [
    "According to this oob evaluation, the BaggingClassifier is likely to achieve 98.9% accuracy on the test set. This can be verified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "db13e97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9895"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69340ebd",
   "metadata": {},
   "source": [
    "We get 98.95% accuracy, which is close to the above 98.85% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19171de5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [0., 1.],\n",
       "       [0., 1.]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# oob decision function (available through the oob_decision_function_ variable)\n",
    "bag_clf.oob_decision_function_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e72a3fb",
   "metadata": {},
   "source": [
    "# Random Patches and Random Subspaces\n",
    "\n",
    "BaggingClassifier supports sampling features, done through \"max_features\" and \"bootstrap_features\"\n",
    "\n",
    "Random Patches\n",
    "- Sampling both training instances and features\n",
    "Random Subspaces\n",
    "- Keeping all training instances (by setting bootstrap=False, max_samples=1.0) but sampling features (bootstrap_features=True OR max_features < (value smaller than 1.0))\n",
    "Sampling features results in more predictor diversity, more bias, and lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ec1d1a",
   "metadata": {},
   "source": [
    "# Random Forests\n",
    "\n",
    "- Random Forest: ensemble of Decision Trees, trained via the bagging method, max_samples set to size of training set\n",
    "\n",
    "- Use RandomForestClassifier class (alternatively, can be done by building a BaggingClassifier and passing it a DecisionTreeClassifier)\n",
    "\n",
    "- RandomForestRegressor: for regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e31b15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses all available CPU cores to train Random Forest classifier with 500 trees (limited to maximum 16 nodes)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db71e6a",
   "metadata": {},
   "source": [
    "# Feature Importance\n",
    "\n",
    "Feature Importance\n",
    "- How much tree nodes that use that feature reduce impurity on average\n",
    "- Weighted average -> each node's weight is equal to the number of training samples that are associated with it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "06583751",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09825657448206183\n",
      "sepal width (cm) 0.024746575681776514\n",
      "petal length (cm) 0.42349326327318154\n",
      "petal width (cm) 0.4535035865629802\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance, using iris dataset\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris[\"data\"], iris[\"target\"])\n",
    "for name, score in zip(iris[\"feature_names\"], rnd_clf.feature_importances_):\n",
    "    print(name, score)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ac867a",
   "metadata": {},
   "source": [
    "Most important features are petal width (45.3%) and petal length (42.3%), while sepal length (9.8%) and sepal width (2.5%) is relatively less important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a7629a",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "Boosting\n",
    "- any ensemble method that can combine several weak learners into a strong learner\n",
    "- trains predictors sequentially, each trying to correct its predecessor\n",
    "- one of popular methods: AdaBoost\n",
    "\n",
    "AdaBoost\n",
    "- pays more attention to the training instances that predecessor underfitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da8b4efb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
